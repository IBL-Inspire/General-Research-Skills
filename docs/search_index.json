[["index.html", "General Research Skills Preface Previous Cohorts", " General Research Skills Dr. F.J. Rodenburg &amp; Dr. H.G.J. van Mil © 2020–2023 Universiteit Leiden Preface Every year there are several students without immediate access to the university course material. In addition, students will lose access to old course material eventually. To address these issues, as well as to improve expectation management, this online version of the entire course is being developed. You can view each day’s lectures, exercises, and references to the books: Rodenburg F.J. (2020). Introduction to Biostatistics Rodenburg F.J. (2021). Elements of Biostatistics Previous Cohorts Did you follow General Research Skills in a previous year? You can find previous iterations below: 2021–2022 2020–2021 This online course is a continuation of the biostatistics portfolio. The portfolio, as well as the online version of the course are a work in progress. Please check back later for updates. Always consult Brightspace for the latest information on the schedule, the course material and the exam. "],["installation-tutorial.html", "Chapter 1 Installation &amp; Tutorial 1.1 Installation Guide 1.2 About RStudio &amp; R Markdown 1.3 About the R Language 1.4 Tutorials", " Chapter 1 Installation &amp; Tutorial Throughout this course, we will use RStudio. To use RStudio, you need to install both R and RStudio. (Even if you have an existing version, I recommend getting the latest version using the guide below.) 1.1 Installation Guide If you’re comfortable with computer software, it boils down to: Install R; Install RStudio; Open RStudio and try to knit a template R markdown or Quarto file. Note about installation problems To ensure a smooth experience throughout the course, address installation issues early on. If you’re following this course in the context of your biology education at Leiden University, you can always contact Dr. H.G.J. van Mil or me about installation issues. Otherwise, see the general tips below, or try asking a question on Stackoverflow. Some general tips that will save you a lot of headaches with software during your education: Setting your system language to English (Windows, Mac) makes everything a lot easier to find; If you store data in Excel, setting the decimal separator to a dot (.) instead of a comma (,) will also save you a lot of extra effort; If you are on Windows, I highly recommend working on an account with administrator rights and setting user account control to “Never notify”. (Link says Windows 7 &amp; 8, but it works the same for Windows 10.) 1.2 About RStudio &amp; R Markdown Watch the video on the RStudio interface below. 1.3 About the R Language There are two great tutorial for learning the basics in R. This video shows how to use them: 1.4 Tutorials Every day there will be exercises in RStudio to teach you how to program, how to run your own analyses and how to interpret output. The optional exercises are for those who want to improve their statistics beyond what is required to pass the course. 1.4.1 Exercise: swirl Watch the part about swirl and complete chapters 1, 2, 4, 7 &amp; 8 from the R Programming course. 1.4.2 Exercise learnr (optional) Many data scientists prefer the tidyverse over base R. This is a collection of packages aimed at writing code in a more ‘tidy’ manner, using the pipeline notation. Complete the Data basics, Filter observations, Create new variables and Summarise Tables chapters from the learnr package as explained in the video. (If you don’t see a Tutorial tab, try running install.packages(\"learnr\") in the console and then restart RStudio.) From here on out, I will assume basic familiarity with R syntax and the RStudio interface. "],["random-variables.html", "Chapter 2 Random Variables 2.1 What are Random Variables? 2.2 Degrees of Freedom 2.3 Probability Distributions", " Chapter 2 Random Variables In this chapter, the concept of a random variable, degrees of freedom and probability distributions is explained. 2.1 What are Random Variables? In empirical science, we draw conclusions about some larger population, using only a limited sample. This process is called inference, and it draws heavily from the definition of a random variable. Though this chapter may seem somewhat abstract, it is important that you have at least some idea what a random variable is. In experiments, random samples are taken to extract particular properties from a population of interest; e.g. we pick randomly 20 students from the population of GRS students in 2021. We will discuss this in more detail in day 2 during the lecture on Experimental Design, but for now assume, as an example, that we are able to pick 20 individuals at random from the population of GRS students from Leiden. In the natural sciences, we might not be interested in these individuals per se, but rather in some properties of interest of the population that can be inferred from this group of 20 individuals; e.g. their height, age, or eye color. In all of these cases this property can be translated into a numeric values; e.g. 180 cm of height, 10 years since birth or enrollment to the university (academic age), and an integer representing the number of brown eyes in our sample (numbers 0–20). The numbers generated by these experiments are called random numbers, and the variable that they represent, like height or the number of brown eyes is called a random variable. Some properties of Random variable of interest relevant in this to this course: We cannot predict the value of a random variable with absolute precision. Every new sample generates new random variable values. e.g. new samples of 20 students can give different numbers of brown eyes. Functions base on random variables are also random variables. The function that calculates the mean takes random variables as input and therefore is also a random variable. New samples can give different means. Because random variables can take different values, there is a distribution associated with it. The distribution of the ages of biology students in Leiden. The distributions of the sample means if we repeat our experiment (important notion in statistics). The distribution of number of brown eyes in sample of 20 students GRS. In the screencast below, I discuss random variables in more detail: To summarize: Random variables (RV) are the starting point of probability theory and statistics. Statistics is built upon a solid foundation of mathematics. The number of RVs can sometimes be expressed in degrees of freedom if statistics like the mean are involved in a calculation. Statistics like the mean, variance, and standard deviation (SD) are random variables, and have their own distributions and statistics; e.g. you can estimate the SD of the distribution of means. Statistics are estimates, how good are our estimates? This can be expressed through the standard error (SE). Can we extract information from random variables? Yes, and you will learn this in this course! The probability theoretical basis is solid, and not vague, but exact. Vagueness in statistics is due to the quality of data (experimental design). Next we address the problem of how we can extract information from random variables. It turns out that statistics has found a way to partition a random variable in a systemic and informative part, and a random part called the error. This will introduce the concepts of degrees of freedom of the statistical model and the random stochastic error. 2.1.1 Exercise Describe random variables in your own words and give some examples of random variables. 2.2 Degrees of Freedom Random variables, degrees of freedom and statistical models. Extracting information from random variables is the main purpose of statistics. By constructing a statistical model that has a close relationship with the hypothesis to be tested (topic of tomorrow), statistics can extract information from data. We start from the idea of the degrees of freedom of a model, and show its relation with the number of observations, thereby introducing the idea of a statistical model and the motivation for applying statistics. To summarize: If \\(n=\\text{df}_{\\text{model}}\\), we have an perfect fit; \\(\\text{df}_\\text{residuals}=0\\) is also known as a saturated model. The RHS of a statistical model, when \\(n &gt; \\text{df}_\\text{model}\\), has two parts: A deterministic part: e.g. \\(\\text{BD} = \\mathbf{\\text{intercept} + \\text{slope} \\cdot \\text{conc}} + \\text{residuals}\\), which is a formal representation of our hypothesis and is informative. A stochastic or random part: e.g. \\(\\text{BD} = \\text{intercept} + \\text{slope} \\cdot \\text{conc} + \\mathbf{\\text{residuals}}\\), representing the properties of the random variable which is non-informative. The residuals represent our ignorance and uncertainty. Ignorance of this type leads to uncertainty: We can have a model for cell division predicting that the cell divides on average after 5 minutes, but will it divide after 5 minutes or will it be earlier or later? The level of our ignorance will affect the level of our confidence in our model. As the data that we sample plays such a crucial role, we must take care when acquiring our data. This is the field of experimental- or study design, the topic of our next lecture tomorrow. 2.2.1 Exercises Describe, in your own words, the relation between a random variable, degree of freedom and a statistical model; Do you agree that the source of vagueness relates to the quality of the data? Write down a argument in favor or against the above statement. 2.3 Probability Distributions A probability distribution describes the chance of different outcomes of a random variable. Also see chapter 6 of Introduction to Biostatistics. What is the chance of being taller than 2 meters? Or what is the highest grade you can expect from random guessing on a multiple choice exam? These are questions that can be answered using a probability distribution. In the video below, three commonly used distributions are explained at a conceptual level. You don’t have to memorize their probability density functions or cumulative density functions for this course. To summarize: A normal distribution is used to model continuous outcomes with a central tendency; A Poisson distribution is used to model independent counts; A binomial distribution is used to model binary data and ratios; All of these distributions are only realistic after accounting for any large, structural differences, and not for typical data; How to account for systematic differences is what you will learn in the lectures on statistical models, particularly regression analysis. 2.3.1 Exercises Why is the normal distribution a good approximation for adult male height in a given year, in a given country? Would it still be a good approximation for adults in a given country? Explain. What does it mean that for the Poisson distribution, the mean is equal to the variance? What is skew? Can the normal distribution be skewed? How about the Poisson distribution, or the binomial distribution? Explain. New (alpha version). We are trying a new concept where you can pick your own set of exercises depending on your interests as a biology student. We have four categories for you to pick from: A—Molecular, Cellular &amp; Medical Biology (exercises); B—Ecology, Evolution &amp; Behavioral Biology (exercises); C—Computational Biology and Bioinformatics (exercises); D—Education &amp; Science Communication (exercises). Pick only one, download the exercises (PDF) and do the exercises. "],["hypothesis-testing.html", "Chapter 3 Hypothesis Testing 3.1 Introduction 3.2 Statistics, test-statistics &amp; hypothesis tests 3.3 One sample t-test: step by step 3.4 Assumptions", " Chapter 3 Hypothesis Testing In this chapter by Dr. H.G.J. van Mil, the concept of a test statistic is explained. 3.1 Introduction After having discussed the basic concepts that motivate the use of statistics and their relation with research questions, hypothesis, experimental design and data, we are now ready to look at hypothesis test based on data. First we investigate, in some detail, the t-test. The reason for this being that the t-test is the work-horse of this course as it will reincarnate in different forms in a variety of tests. But the t-test illustrates elements of statistic testing that is common in other test-statistics. Chapter 4 deals with the morning lecture and exercises, whereas the afternoon lectures and exercises are displayed in chapter 5.1 and deals with the practical application of the t-test. 3.2 Statistics, test-statistics &amp; hypothesis tests This paragraph shortly revisit the standard statistics of the population and sample mean and variance to explain the concept of normalization; the normalization occurs though the sample size or the degree of freedom. This is important because a good normalization allows us to compare different system with each other. For instance, it makes no sense to compare the sums of squares of two sample with unequal sample size (why?). However, we can compare the normalized sum of squares like the variance also known also the mean sum of squares. The method of normalization is also applied in test-statistics like the t-test. Once this normalization is combined with the properties of the null-hypothesis (no effect), the actual t-test is made possible and the the t-distribution can be derived. In the screencast below we will make this link The line of reasoning used. Compare one mean to a value or two means with each other (effect size). Normalize the effect size with a measure for the reliability of the estimates sample means (SE). Being a random variables, the t-value has a distribution (t-distribution). Utilizing the properties of the null-hypothesis that the expected effect size is 0, we can derive that expected t-value is 0 with a variance of 1. This is true for all data involved in t-test We can use the t-distribution, in conjunction with the t-value and the degrees of freedom to calculate \\(P(\\text{Data}|H_0)\\). or in a more formal way \\(\\overline{y}_{A} - \\overline{y}_{B} = \\text{effect size}\\) \\(\\frac{\\overline{y}_{A} - \\overline{y}_{B}}{SE} = t\\) \\(\\frac{\\overline{y}_{A} - \\overline{y}_{B}}{SE} = t \\implies \\frac{N(\\overline{y}_{A} - \\overline{y}_{B}, SE)}{SE} = t\\text{-distribution} = \\frac{\\Gamma ( \\frac{\\text{df}+1}{2}) }{\\sqrt{\\text{df}\\cdot\\pi}\\Gamma( \\frac{\\text{df}}{2})}(1+\\frac{t^2}{\\text{df}})^{-\\frac{\\text{df}+1}{2}}\\) \\(\\frac{\\overline{y}_{A} - \\overline{y}_{B}}{SE} = t_{H_0} \\implies \\frac{N(0, SE)}{SE} = t\\text{-distribution}(t,\\text{df}|\\alpha)\\) 3.3 One sample t-test: step by step In this screencast we will look closer to the output of the R function for the t-test. First I will perform all the individual steps and than use the t.test function in R. The actual use of this function will be discussed in more detail in paragraph 5.1 Question: If we would perform another experiment producing a new confidence interval which includes 82 mmHG, what would that mean in relation to the population mean? 3.4 Assumptions In all statistical test we make some kind of assumptions. For instance because the t-distribution is derived from a normal distribution, we make an explicit assume that that is the case. As we shall see, all the methods discussed in this course have assumption that need to be tested. In the case of the t-test we can test the assumption before the actual test is performed, for the other test we can only test the assumption after we run the test. Question: Why do we need to test if the sample distribution is similar to a normal distribution? Why do we tests for outliers? "],["statistical-tests.html", "Chapter 4 Statistical Tests 4.1 T-Test 4.2 Chi-Squared Test 4.3 Multiple Testing", " Chapter 4 Statistical Tests How do you perform basic comparisons of samples? How do you calculate test statistics, what do they mean, and when is something significant? This chapter covers everything you need to know about basic significance tests for GRS, including the \\(t\\)-test, \\(\\chi^2\\)-test, and multiple testing correction. 4.1 T-Test A test for comparing two group means. The data-generating process is assumed to follow a normal distribution, given the group. 4.1.1 Choosing the right \\(t\\)-test Watch part 1 of the \\(t\\)-test lecture and answer the questions below. 4.1.2 Exercises Suppose you measured the beak dimensions (length, depth) of three Geospiza fortis and three Geospiza scandens finches. Copy the code below to enter the example data into R: fortis_length &lt;- c(11.00, 10.60, 11.43) fortis_depth &lt;- c(9.70, 9.30, 10.28) scandens_length &lt;- c(12.90, 14.20, 14.00) scandens_depth &lt;- c(7.90, 9.10, 8.80) Is there a significant difference in mean beak length at \\(\\alpha = 0.05\\)? Use an appropriate \\(t\\)-test. Is there a significant difference in mean beak depth at \\(\\alpha = 0.05\\)? Use an appropriate \\(t\\)-test. If you use a Wilcoxon test instead of a \\(t\\)-test, how does this affect your conclusion? Suppose you measure three juvenile G. fortis finches, tag them, and then measure them again once they reach adulthood: juvenile_length &lt;- c(8.69, 7.69, 8.11) juvenile_depth &lt;- c(9.44, 7.22, 8.38) adult_length &lt;- c(11.50, 9.70, 10.40) adult_depth &lt;- c(9.50, 8.00, 8.80) Is there a significant difference in mean beak length at \\(\\alpha = 0.05\\)? Use an appropriate \\(t\\)-test. Is there a significant difference in mean beak depth at \\(\\alpha = 0.05\\)? Use an appropriate \\(t\\)-test. 4.1.3 Understanding how the \\(t\\)-test works Watch part 2 of the \\(t\\)-test lecture and answer the questions below. 4.1.4 Exercises Interpreting output is an important part of the exam. The questions on the exam will usually be no harder than this, so if you get these right, your understanding of the subject matter is sufficient. (If you want to review specific parts of the video, there are chapters if you watch it in a separate window.) Below is the (partial) output of a \\(t\\)-test. What kind of \\(t\\)-test is this? t = -1.8608, df = 17.776, p-value = 0.07939 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -3.3654832 0.2054832 sample estimates: mean in group 1 mean in group 2 0.75 2.33 Use the output shown above to determine what the standard error (\\(\\text{SE}\\)) was. Below is the output of another \\(t\\)-test on the same data. What kind of \\(t\\)-test was used here? t = -1.8608, df = 17.776, p-value = 0.0397 alternative hypothesis: true difference in means is less than 0 95 percent confidence interval: -Inf -0.1066185 sample estimates: mean in group 1 mean in group 2 0.75 2.33 Compare the \\(p\\)-value of the first and second \\(t\\)-test. What do you notice? What was the total sample size of the data used in the \\(t\\)-test below? t = -4.9005, df = 38, p-value = 1.811e-05 alternative hypothesis: true difference in means is not equal to 0 95 percent confidence interval: -8.994387 -3.735613 sample estimates: mean in group 1 mean in group 2 19.735 26.100 (hard) Using your answer from (5) and the output, can you calculate the standard deviation (\\(s\\))? You may assume equal sample sizes. (If you get stuck, have a look here.) (hard) A set of extra exercises can be downloaded here and its required data sets here. (If you can’t knit, click here for a PDF version of the exercises.) 4.2 Chi-Squared Test A test for comparing observed to expected frequencies (counts). Can also be used for contingency tables. 4.2.1 Lecture Watch the lecture on the \\(\\chi^2\\)-test and answer the questions below. 4.2.2 Exercises Suppose a vaccine is reported to prevent \\(95\\%\\) of virus infections in healthy adults. The vaccine is given to \\(1000\\) healthy adults, all of which are expected to be exposed to the virus. If \\(63\\) individuals end up being infected, is there a significant deviation from the reported effectiveness at \\(\\alpha = 0.05\\)? Below is a \\(2 \\times 2\\) contingency table: male female left-handed 59 455 right-handed 40 479 With \\(\\alpha = 0.05\\), is there a difference in the \\(\\text{male}:\\text{female}\\) ratio of left and right-handed individuals? For larger contingency tables, a \\(p\\)-value alone does not quite paint the picture of what is going on. This is why there have been quite a few attempts to make visual summaries of categorical data. One such example is a mosaic plot. The code below generates a mosaic plot for the hair and eye color frequencies of \\(592\\) individuals.1 Copy the code to R and run it. Can you tell which combinations are less common in females than in males? mosaicplot(HairEyeColor) 4.3 Multiple Testing If a study involves testing more than one hypothesis, then there is an increased chance of a false positive. 4.3.1 Lecture Watch the lecture on multiple testing and answer the questions below. 4.3.2 Exercises Suppose a paper compares \\(10\\) different diets with paired \\(t\\)-tests (before and after), to find out which diets result in a significant reduction in fat mass. If a level of significance \\(\\alpha = 0.05\\) is used and no multiple testing correction is applied, what is the chance of at least one false positive? Suppose the results of the paper described in the previous question are the ten \\(p\\)-values shown below. Copy the code to your R markdown file and answer the following questions: pvalues &lt;- c(`diet 1` = 0.06936, `diet 2` = 0.81778, `diet 3` = 0.94262, `diet 4` = 0.26938, `diet 5` = 0.16935, `diet 6` = 0.03390, `diet 7` = 0.17879, `diet 8` = 0.64167, `diet 9` = 0.02288, `diet 10` = 0.00832) Which diets have a significant effect after Bonferroni correction? Which diets have a significant effect after FDR correction? Why are some \\(p\\)-values equal to \\(1\\) after correction? There have been many criticisms of the over-reliance of scientific papers on \\(p\\)-values. This is in part because there are many incorrect interpretations being used in papers, and in part because problems like multiple testing, stopping rules and stepwise regression are often left unaddressed. This is one of the major reasons for the ongoing reproducibility crisis. Which of the following interpretations are incorrect? The \\(p\\)-value is: The chance of a false positive; The chance that the null-hypothesis is false; The chance that the null-hypothesis is correct; The chance that the alternative hypothesis is false; \\(1\\) minus the chance that the alternative hypothesis is correct; The probability that the results arose by chance; The chance that the test statistic is this large, or larger, if the null-hypothesis were correct; If the null-hypothesis were true, and the experiment were repeated a large number of times, then the \\(p\\)-value is the expected proportion of experiments with this larger, or larger a test statistic. A solution to the reproducibility crisis proposed by some groups is to lower the commonly used standard of \\(\\alpha = 0.05\\) to some lower value, like \\(\\alpha = 0.005\\). Why is this solution problematic? What would be a better solution? Below is a comic from xkcd.com. What is meant by this comic? Snee, R. D. (1974): Graphical display of two-way contingency tables. The American Statistician, 28, 9–12. doi: 10.2307/2683520.↩︎ "],["study-design.html", "Chapter 5 Study Design 5.1 Lecture", " Chapter 5 Study Design In this chapter, we explain statistical considerations of experimental design that ensure maximal power and generalizability. 5.1 Lecture “To consult the statistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of.” — Sir Ronald Aylmer Fisher If you want help with proper randomization schemes during your internship, several examples with R code are given in paragraph 7.1 of Introduction to Biostatistics "],["anova.html", "Chapter 6 ANOVA 6.1 Overview 6.2 How does ANOVA work? 6.3 Two-Way, Multi-Way &amp; Interaction 6.4 Where is part 4? (*)", " Chapter 6 ANOVA Analysis of variance (ANOVA) is a model for comparing any number of group means. The observations are assumed to be independent, deviations from the group means are assumed to follow a normal distribution, and group variances are assumed to be equal. 6.1 Overview Watch part 1 of the ANOVA lecture and answer the questions below. Summary of the example in the video Here is a summary if you want to reproduce the example analysis: 1. Enter the data: SBP &lt;- c(121, 122, 114, 124, 118, 127, # thiazide 126, 128, 118, 130, 132, 124, # calcium channel blockers 145, 145, 139, 133, 146, 150) # placebo treatment &lt;- gl(n = 3, k = 6, labels = c(&quot;thiazide&quot;, &quot;CCB&quot;, &quot;placebo&quot;)) DF &lt;- data.frame(treatment, SBP) 2. Plot the data: plot(SBP ~ treatment, data = DF) 3. Fit an ANOVA: ANOVA &lt;- aov(SBP ~ treatment) 4. Perform visual diagnostics: require(&quot;car&quot;) # Install if missing par(mfrow = c(2, 2)) # Plot in a 2x2 grid plot(ANOVA, which = 1) # Residuals vs fitted qqPlot(ANOVA, reps = 1e4) # QQ-plot. See footnote for errors plot(ANOVA, which = 3) # Scale-location plot(ANOVA, which = 4) # Cook&#39;s distance par(mfrow = c(1, 1)) # Restore the default 5. Check the omnibus test: summary(ANOVA) 6. If the omnibus test is significant, you can perform a post hoc analysis: posthoc &lt;- TukeyHSD(ANOVA) print(posthoc) plot(posthoc) 6.1.1 Exercises The PlantGrowth data set is included with the default installation of R. It contains \\(30\\) independent measurements of dry weight from plants subjected to three types of treatment. You can view it by running View(PlantGrowth) in the console, or you can see the structure of the data and produce a summary of the data like this: str(PlantGrowth) summary(PlantGrowth) Make a plot showing the weight of the different groups. You can use the same kind of plot as shown in the video. Fit an ANOVA and save it as an object called ANOVA. Here is the code I used in the video to perform visual diagnostics. Run it and comment on the plots.2 require(&quot;car&quot;) # Install if missing par(mfrow = c(2, 2)) # Plot in a 2x2 grid plot(ANOVA, which = 1) # Residuals vs fitted qqPlot(ANOVA, reps = 1e4) # QQ-plot. See footnote for errors plot(ANOVA, which = 3) # Scale-location plot(ANOVA, which = 4) # Cook&#39;s distance par(mfrow = c(1, 1)) # Restore the default Perform the omnibus test. Is there a difference in weight among any of the groups? Perform a post hoc analysis. Which group means differ significantly? 6.2 How does ANOVA work? Watch part 2 of the ANOVA lecture and answer the questions below. 6.2.1 Exercises Are the following statements true or false? The omnibus test is used to see which of \\(k\\) group means differ; The omnibus test is an \\(F\\)-test; The residual variance is smaller than the total variance in the outcome; The \\(p\\)-value is the chance of obtaining an \\(F\\)-value at least this large if there were no differences in group means; The post hoc of a one-way ANOVA must still be corrected for multiple testing. Below is the output of an omnibus test of some data: Df Sum Sq Mean Sq F value Pr(&gt;F) Species 2 63.21 31.606 119.3 &lt;2e-16 *** Residuals 147 38.96 0.265 What is the number of groups? What is the total sample size? What does Pr(&gt;F) mean? Here is another omnibus test, but with some values omitted: Df Sum Sq Mean Sq F value Pr(&gt;F) Group x 3488 697.5 z 0.000277 *** Residuals 48 5745 y What is the value of x? What is the value of y? What is the value of z? Why do we bother with an omnibus test, why not just use the post hoc immediately after fitting the model? 6.3 Two-Way, Multi-Way &amp; Interaction Watch part 3 of the ANOVA lecture and answer the questions below. 6.3.1 Exercises In an experiment on rats, a researcher wanted to find out what the effect would be of the genotype of the biological mother and the foster mother on the growth of the rats.3 \\(61\\) rat litters were taken from their biological mother and placed with a foster mother. See the figure below. Which is suitable here: One-way, two-way, or multi-way ANOVA? Do you expect an interaction or main effects here? Explain. Based on your answer to the previous question, run an appropriate ANOVA for this data set. Perform visual diagnostics, look at the summary, and perform a post hoc analysis. Write a short conclusion. The data set is called genotype and you can access it by loading the package MASS: require(&quot;MASS&quot;) # Comes preinstalled with R str(genotype) summary(genotype) (hard) Search online for the definition of the central limit theorem. Is normality of the residuals a reasonable assumption for this study from a theoretical point of view? (HINT: Look at the help page for the genotype data set and read the description of the outcome variable.) 6.3.2 Additional Exercises (*) The iris data set is included with the default installation of R. It contains \\(150\\) independent measurements of sepal and petal dimensions from three types of Iris species. Choose one of: Sepal.Length, Petal.Length, Sepal.Width and Petal.Width and perform a complete analysis to find out whether species differ on average. In this exercise we are going to calculate how much variance your ANOVA from the previous exercise has explained. Remember the animation from the video: Estimate the total variance in the outcome using var(); Estimate the residual variance. You can obtain the residuals of a model using residuals() on the fitted model; The variance explained is one minus the proportion of the total variance that remains after fitting the model. How much variance did your model explain? If your model is called ANOVA, then you can check your answer as follows: summary(lm(ANOVA))$r.squared. Did you get the calculation right? This value will be explained again in the linear regression lectures. 6.4 Where is part 4? (*) When you use two- or multi-way ANOVA, Tukey’s Honest Significant Difference only corrects for the categories within each explanatory variable. This quickly becomes problematic: If you’ll recall from the multiple testing lecture, with \\(\\alpha = 0.05\\), with as little as three explanatory variables, you will already have a chance of \\(1 - (1 - 0.05)^3 \\approx 14.3\\%\\) chance of at least one false positive among all comparisons. In order to correct for this, you would need to somehow obtain uncorrected \\(p\\)-values from TukeyHSD and then perform multiple testing correction for the total number of comparisons. This is unfortunately not very easy to do, so Dr. van Mil and I decided that it would not be part of the GRS course. We will include this in the Advanced Statistics course for master students. If you are interested, or need this for your internship, please have a look at the vignettes of the emmeans package, or wait for me to release part 4 after the course. What a vignette is, is covered in the video on help files. List of problems and how to fix them: (1) If you get an error about a missing package, install car using install.packages(\"car\"), then run diagnostic plots code again. (2) If you can’t install car, replace qqPlot(ANOVA, reps = 10e4) with: plot(ANOVA, which = 2). (3) If you get an error about figure margins being too small, simply enlarge your plot pane as shown here, then run the diagnostic plots code again.↩︎ Bailey, D. W. (1953): The Inheritance of Maternal Influences on the Growth of the Rat. Unpublished Ph.D. thesis, University of California. Table B of the Appendix.↩︎ "],["simple-linear-regression.html", "Chapter 7 Simple Linear Regression 7.1 Overview 7.2 Step-by-Step Walkthrough 7.3 Model Diagnostics 7.4 Transformation", " Chapter 7 Simple Linear Regression A model for a continuous response variable and a continuous explanatory variable, between which a linear relationship is assumed. The data-generating process is assumed to follow a normal distribution, given the effect of the explanatory variable. 7.1 Overview Here is the illustration of the intercept and slope: Here is the illustration of the residuals: 7.1.1 Summary of the example in the video 1. Enter your data: (you can use other methods) BMI &lt;- c(18.4, 31.2, 22.7, 21.6, 27.0, 27.1, 17.5, 20.9, 26.6, 27.6, 25.2, 25.1, 25.7, 26.1, 32.4, 31.6, 17.2, 29.1, 32.9, 20.6) BFP &lt;- c(18.9, 26.0, 22.1, 13.2, 17.3, 22.9, 13.9, 29.2, 35.4, 20.6, 30.3, 32.1, 27.2, 28.2, 38.2, 31.3, 15.2, 36.7, 24.8, 18.3) DF &lt;- data.frame(BMI, BFP) 2. Plot your data plot(BFP ~ BMI, data = DF) 3. Fit a simple linear model: LM &lt;- lm(BFP ~ BMI, data = DF) 4. Perform visual diagnostics: require(&quot;car&quot;) # Install if missing par(mfrow = c(2, 2)) # Plot in a 2x2 grid plot(LM, which = 1) # Residuals vs fitted qqPlot(LM, reps = 1e4) # use plot(LM, which = 2 in case of errors) plot(LM, which = 3) # Scale-location plot(LM, which = 5) # Cook&#39;s distance vs leverage par(mfrow = c(1, 1)) # Restore the default 5. Check the regression table: summary(LM) 6. Plot your model and write a conclusion about it: plot(BFP ~ BMI, data = DF) lines(coef(LM)) text(32.5, 12.5, bquote(R^2 == .(round(summary(LM)$r.squared, 3)))) 7. You could add a prediction interval: plot(BFP ~ BMI, DF) x_values &lt;- seq(15, 35, 0.1) # 15, 15.1, 15.2, ..., 35 newdata &lt;- data.frame(BMI = x_values) y_predict &lt;- predict(LM, newdata, interval = &quot;predict&quot;, level = 0.95) lines(y_predict[, 1] ~ x_values) # Regression line lines(y_predict[, 2] ~ x_values, lty = 2) # Lower bound lines(y_predict[, 3] ~ x_values, lty = 2) # Upper bound 7.1.2 Exercises A set of exercises can be downloaded here and its required data sets here. (If you can’t knit, click here for a PDF version of the exercises.) 7.2 Step-by-Step Walkthrough Here is a comprehensive tutorial by Dr. H.G.J. van Mil, using simulated data: You can watch the video while trying to reproduce the steps. You can change some of the simulation input if you like: Simulation code: set.seed(635) n &lt;- 20 Intercept &lt;- 220 Slope &lt;- 3.5 Phosphate &lt;- runif(n, 0, 10) Residuals &lt;- rnorm(n, 0, 3) Biomass &lt;- Intercept + Slope * Phosphate + Residuals SimDat &lt;- data.frame(Biomass, Phosphate) From there on you will have an object in your workspace called SimDat that contains the simulated data set. You can use it to follow the steps shown in Harald’s video. 7.3 Model Diagnostics 7.3.1 Exercises Below is a famous standard data set in R on the distance until a car has stopped completely from the moment it starts braking.4 Its variables are: dist: numeric stopping distance (ft) speed: numeric speed (mph) data(cars) Fit a simple linear model and perform visual diagnostics. Is this an appropriate model? Why? Fit the following model with a square-root transformation of the outcome and compare the diagnostic plots. Which do you think is better and why? LM2 &lt;- lm(sqrt(dist) ~ speed, cars) Complete the analysis using the example analysis. Report a conclusion. 7.4 Transformation In yesterday’s rice and fruitfly data we observed that the assumption of homoscedasticity is not correct for these data. In the lecture on model diagnostics, transformation was briefly discussed. In this screencast, Harald discusses transformation in the context of the fruitfly data: Ezekiel, M. (1930) Methods of Correlation Analysis. Wiley.↩︎ "],["factorial-anova.html", "Chapter 8 Factorial ANOVA 8.1 Interaction 8.2 Three example factorial ANOVA data sets 8.3 Model specification and diagnostics 8.4 Model selection 8.5 Some final thoughts and visualisation", " Chapter 8 Factorial ANOVA A model for comparing all combinations of categorical variables. In these set of lectures we move from models with one explanatory variable, one-way ANOVA and simple linear regression (Y ~ F and Y ~ X), to multiple regression models (e.g. Y ~ F1 + F2 + F1:F2 or Y ~ F + X + F:X) with more than one explanatory variable; with F a factor and X a numerical variable. This will introduce two new aspects to our analysis: Interaction between the explanatory variables: The effect of one explanatory variable on the response variable depends on the values of the second explanatory variable and vice versa; e.g. a treatment might have different effect in different species. More models can be fitted on the data and we need to select the model that fits that data best; e.g. the follow option are possible: Y ~ F1 + F2 + F1:F2 + residuals Y ~ F1 + F2 + residuals Y ~ F1 + residuals Y ~ F2 + residuals Y ~ residuals In the first screencast I will explain the concepts of interaction and the difference between factorial ANOVA and ANCOVA. The next screencast explains why model selection is in order by analyzing factorial ANOVA on three different data set, One data set representing the null hypothesis, One data set representing data with interaction, and finally and data set representing data with interaction. In doing so we can investigate the different criteria for selecting the “best” model. 8.1 Interaction In this screencast I argue why we might need additional degrees of freedom to analysis data with interactions between the explanatory variables. For example, F1:F2 compound variable in Y ~ F1 + F2 + F1:F2 can be read as that F1 modulates the effect of F2 on Y (or equivalently, the effect F2 has on how the F1 affects Y). In order to fit data with interaction, we need more degrees of freedom. 8.2 Three example factorial ANOVA data sets In this screencast I introduce three data sets to explain the different qualities of factorial ANOVA model: One data set results in a so-called null-model, a model where none of the treatment combinations affect the response variable (\\(H_0\\)). One data set that belongs to a factorial ANOVA model without interaction between the two explanatory factors F1 and F2. One data set that belongs to a factorial ANOVA model with interaction between the two explanatory factors. here we need the most complex model also known as the maximal model, to fit the data correctly. It is shown that the maximal model with interaction needs more degrees of freedom (is more complex), than the model without interaction, and that the model without interaction needs more degrees of freedom than the null-model 8.3 Model specification and diagnostics In this screencast the model specification for the three data set are discussed together with the model diagnostics. It is shown that it follows the same steps as the one-Way ANOVA discussed in chapter 6, because we only need to analyze the behavior of the residuals of the different models, a property shared by all statistical models. 8.4 Model selection After it was shown in the previous screencast that the data seem to adhere to the assumptions of the model, the search for the best model for each individual data set can start. The model selection procedures results in three different models for the three different data sets. In one of the screencast of Wednesday we will go into more detail of model selection. 8.5 Some final thoughts and visualisation Here we will look what the effect can be on the model diagnostics plots when the factorial ANOVA model is miss-specified; e.g. when one term (or degree of freedom) is missing. Also some data visualizations of factorial ANOVA are briefly discussed. "],["ancova-model-selection.html", "Chapter 9 ANCOVA &amp; Model Selection 9.1 Introduction 9.2 The ANCOVA explained by simulations 9.3 The analysis of ANCOVA data step by step 9.4 Model Selection", " Chapter 9 ANCOVA &amp; Model Selection ANCOVA is multiple linear regression with both categorical and continuous explanatory variables. Model selection is a set of guidelines for choosing the right model. The best model depends on what the model is intended to be used for. 9.1 Introduction We now turn our attention to to the case where we have at least one numerical and at least one factor as explanatory variable. These type of models we call ANCOVA or ANalysis of COVAriance and a simple example is the model Y ~ F + X + F:X with F a factor and X a numeric variable. Most of the ground work has been covered in the screencasts on one-way ANOVA, linear regression and factorial ANOVA. Therefore this set of screencasts on ANCOVA are more applied, following the same steps as the factorial ANOVA and the differences are emphasized where needed. Try to identify the similarities and differences between these multiple regression models. The first screencast discusses the difference between the interpretation of an ANCOVA model as compared to an factorial ANOVA model. The second screencast runs trough the analysis of the of an ANCOVA step by step. 9.2 The ANCOVA explained by simulations Simulations are used to explain the challenges faced when analyzing ANCOVA; when are interaction’s significant in the ANCOVA context and what does this mean. First we simulate the same model without interaction multiple times using different set.seed() values to the investigate the uncertainty in the estimates of the slopes. Then we simulate a model with interaction. 9.3 The analysis of ANCOVA data step by step An ANCOVA analysis is performed step by step follow the tutorial for ANCOVA. In this screencast the differences with a factorial ANOVA are emphasized. 9.3.1 Exercises Below part of the output of a ANOVA analysis. Residuals: Min 1Q Median 3Q Max -0.8528 -0.3010 0.0563 0.2708 0.8555 Coefficients: Estimate Std. Error t value Pr(&gt;|t|) (Intercept) 5.09856 0.36516 13.963 4.25e-11 *** TreatB 1? 0.51641 -1.405 0.17712 TreatC -1.30794 2? -2.533 0.02084 * Concentration 0.06073 0.08729 0.696 0.49547 TreatmentB:Concentration 0.43544 0.12345 ?3 0.00241 ** TreatmentC:Concentration 0.24936 0.12345 2.020 0.05852 . --- Signif. codes: 0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1 Residual standard error: 0.5657 on 18 degrees of freedom Multiple R-squared: 0.7831, Adjusted R-squared: 0.7229 F-statistic: 13 on 5 and 18 DF, p-value: 1.892e-05 Calculate the missing numbers ?1, ?2 and ?3. How much of the variance is explained by this model? What was the total sample size of this experiment? Based on this output, do you think there is an interaction? Explain. What is the estimate for the intercept of TreatA? What about TreatB and TreatC? Draw what this model would look like. You may use R, or pen and paper. 9.4 Model Selection 9.4.1 Exercises A set of exercises can be downloaded here and its required data set here. (If you can’t knit, click here for a PDF version of the exercises.) 9.4.2 Exercises (hard) In order to study the effect of caviar on health, a survey is distributed in a city on whether the respondents have ever eaten caviar, and if so, how frequently they eat caviar. After receiving the responses, 100 random individuals from each of the following groups are invited for a health check-up: Group A: Has never consumed caviar; Group B: Has tried caviar once, or a few times in their life; Group C: Eats caviar about once per year; Group D: Eats caviar multiple times per year; Group E: Eats caviar about once per month. What problems do you think there might be with this study design? (HINT: There are at least two major flaws.) Can you come up with a better design for the research question? Can you come up with a minimal sample size needed to conduct this research? You can estimate the minimal required sample size for both the original design and your own version. "],["pca.html", "Chapter 10 PCA 10.1 Introduction 10.2 Motivation behind PCA 10.3 PCA fundamentals 10.4 PCA in classification 10.5 PCA and gradients", " Chapter 10 PCA An exploratory technique for explaining the variance among a group of continuous variables, by creating linear combinations. 10.1 Introduction Principal Component Analysis, or PCA in short, is a dimension reduction method or unsupervised machine learning method in modern statistical language. PCA is an exploratory method and need to be contrasted with conformational method’s like the methods discussed in the previous two weeks, and prediction methods, that are not discussed in this course. Exploratory methods are used when there is no clear enough understanding of our system of interest to generate an meaningful hypothesis; a hypothesis for conformational statistics. One example is the measurement of many morphological or genetic features of different Species within a genus. The dataset is than explored to identify those features that that can be used to classify or reltaed species to each other. Now imagine a spreadsheet with individual subjects as rows and features or properties as columns. An example could be a dataset within each row an ID representing an individual, followed by 250 features measured on that individual; in the context of dimension reduction (PCA) this dataset contains 250 dimensions. Its clear that we cannot visualize 250 dimensions effectively in order to explore the dataset, at least not as in the case of a two dimensional setting (x,y) used in the previous weeks. As a dimension reduction method, PCA reduces the dimensions of the problem, 250 dimensions in our example, to a two dimensional space that can be represent in a x,y plot that we can investigate on paper (and also the flat computer screen). In doing so it tries to extract the maximal amount of information from the higher dimensional space and concentrate it in two dimensions. Information used to accomplish this takes the following statistics measures, Variance: If a variable (dimensions) varies a lot in you data, it means that its dimension has discriminating properties and caries information. The opposite, no variation, would mean that the property is shared by all individuals and no groups of individuals can be singled out, therefore this dimension would not carry any information. Correlation: If variables (dimensions) vary, then they can correlate. If features are correlated, then we might expect that there is some morphological or functional relations and that feature therefore caries information. We shall work out these ideas in the following screencasts. The fundamentals of PCA are however, based on linear algebra which is unfortunately outside the scoop of this course. Therefore we will take a more intuitive approach in the screencasts, using the concepts introduced in the previous two weeks, the aforementioned variance and correlations. 10.2 Motivation behind PCA This screencast explains the basic motivation behind dimensions reduction PCA. First the explanation behind the dimension reduction of a 3D object into 2D. It also addresses the meaning of the modern classification of PCA as an unsupervised machine learning methodology by comparing it with supervised machine learning. As an simple example the Iris data is used. This data set will we analyzed in more detail in another screencast. Questions: What is the difference between supervised and unsupervised machine learning methodologies. 10.3 PCA fundamentals PCA algorithm is solidly based on linear algebra. Without going in to detail we will use a concept and two measures from linear algebra: Orthogonality: Dimensions (variables) are independent of each other if they are Orthogonal; geometrically we would visualize that as perpendicular dimensions (axes). Eigenvectors: The directions of these orthogonal axes, and thus the principal components, are given by the eigenvectors. Eigenvalues: The length, or weights, of the eigenvectors are given by the eigenvalues. It turns out that the eigenvalues are directly related to the variance of the principle component. PCA captures the most variance in the first new axes, the first principal component, by capturing most of the information (variance or eigenvalues). This is repeated for the next orthogonal principal component by capturing the most information (variance or eigenvalue) that is left after the previous step. These steps are repeated until we have the same number of principle components as dimensions. How we need to interpret the loading of a variables in a principal component? 10.4 PCA in classification In this simple example the Iris data, introduced earlier, are analyzed using PCA. Note that normally we do not have the information on which species the individuals belong to, unsupervised. The species are only used here for a didactic purpose. Questions: Which of the Sepal or Petal width and lengths were important in the first and which in the second principal component? Which feature(s) (variable(s)) is response for the separation between Versicolor and Virginica? 10.5 PCA and gradients The Heptathlon dataset give the scores, for every athlete, for the different disciplines of the woman’s Olympic Heptathlon of 1988 in Seoul. The row are the individual athletes and the columns the scores per discipline. This dataset might be interesting for the coaches to investigate if one of disciplines might contribute more to a success than another. Here we do not look in particular to clusters of groups of athletes but if the distribution of the individual athlete is related to the final score when visualizing the principal components. The relation with linear regression and supervised learning is also discussed. Question Explain in your own words which disciplines contributed to the success of Jackie Joyner-Kersee. 10.5.1 Exercises A set of exercises can be downloaded here and its required data sets here. (If you can’t knit, click here for a PDF version of the exercises.) PCA is also covered in Elements of Biostatistics, chapter 4. A demonstration in R can be found in paragraph 4.4. "],["bioinformatics-workshop.html", "Chapter 11 Bioinformatics Workshop 11.1 Preparation 11.2 RNAseq 11.3 Exercises", " Chapter 11 Bioinformatics Workshop In this special chapter, bioinformatician Dr. Bastienne Vriesendorp will show you a real analysis of high-dimensional transcriptomics data (RNAseq). We will use various techniques that you have learned throughout the course, including PCA and multiple testing correction. 11.1 Preparation This workshop requires packages from a specialized repository for bioinformatics. Since this is prone to installation problems, please try installing the following packages before the workshop. We are happy to help you with any installation issues. install.packages(c(&#39;ggplot2&#39;,&#39;plyr&#39;,&#39;dplyr&#39;,&#39;mvdalab&#39;, &#39;ggfortify&#39;, &#39;stringr&#39;)) # The last package is not installed from CRAN (the usual place we install from), # but from a specialized repository for bioinformatics called Bioconductor: install.packages(&quot;BiocManager&quot;) BiocManager::install(&quot;DESeq2&quot;) 11.2 RNAseq (…) to be included. 11.3 Exercises Download the R markdown file and the both data sets (raw counts and meta data). Save all three files in the same location, then open the R markdown file. This file contains all instructions required for the workshop. "],["glm.html", "Chapter 12 GLM 12.1 Preparation 12.2 Introduction 12.3 Poisson GLM 12.4 Binomial GLM 12.5 Exercises: Choose the Probability Distribution", " Chapter 12 GLM A model for a response variable and any number of explanatory variables (continuous and/or categorical). The data-generating process can follow any distribution in the exponential dispersion family. 12.1 Preparation To properly understand the generalized linear model, you should by now be familiar with: Probability distributions; Simple linear regression Multiple linear regression &amp; ANCOVA If you want to prepare for this chapter before the lectures on GLM, I recommend revisiting those subjects. 12.2 Introduction Data do not always abide to the assumptions of our statistical models. This might be due to extreme values in our data, which might be outliers, or due to bad experimentation. But there are also many instances where the deviation from the assumptions are due to more data fundamental properties. We have observed this in the fruitfly and rats data sets. Longevity and survival can be interpreted as the time we need to wait until the fruit flies or rats dies. Most often waiting times are not distributed as a normal distribution. Counting processes also result in non-normal distributions (Poisson). The same is true if our response variable is not numeric but for instance a factor with two levels; e.g. the present or absence of a particular object, success or failure, dead or alive, sprouting or not, gene on or off, to name a few. These two-valued response data are called binomial data and the related distribution is the binomial distribution. Waiting times, counts and binomial data occur commonly in life science research. In order to accommodate for the analysis of these data, we need to generalize the linear model in such a way that will allow for the statistical analysis of Poisson and binomial data and keeps the ease of interpretation of the linear models. The method that was developed is called the generalized linear model, or GLM. Below we will discuss two instances of GLMs, one for Poisson and one for binomial data. Beside a small addition in the coding, the GLM transforms the model rather that the response variable, this will allow us to give a standard interpretation to intercepts and in particular the slopes. GLM is also discussed in chapter 3 of the book Elements of Biostatistics. 12.3 Poisson GLM In this first screencast we give the motivation for choosing a GLM approach to Poisson data. I also discuss the link between the different distributions. It turns out that the three distributions normal, Poisson, and binomial, all belong to the same family of distribution, the exponential dispersion family. It’s important to note that both the Poisson and binomial distributions have only one free parameter: \\(\\lambda\\) and \\(p\\), respectively, whereas the normal distribution has two: \\(\\mu\\) and \\(\\sigma\\). Question: How are distribution parameters of the normal distribution, \\(\\mu\\) and \\(\\sigma\\), related to the parameter of the Poisson distribution \\(\\lambda\\) and the parameters of the binomial distribution, \\(p\\) and \\(n\\)? 12.3.1 Model specification Because GLM models can deal with the different distributions from the exponential family, we need to make explicit which family member we need to select, e.g. Poisson for counts and binomial for two level factors. One example for a GLM encoded in R: Model &lt;- glm(Counts ~ Treatment + Concentration + treat:Concentration, family = poisson(link = &quot;log&quot;) Note that you can identify: Linear equation: Counts ~ Treatment + Concentration + treat:Concentration Exponential family member: poisson Link function: link = \"log\" Further note that we still have a linear model. Therefore we will run through all the steps that we did in the analysis of linear models. Question: Name three link function for the Poisson distribution. 12.3.2 Analysis of GLM data and interpretation In the GLM we transform the model in order to to get the residuals to behave as a normal distribution again. The assumption of homoscedasticity is not relevant for GLM, as heteroscedasticity is a property of the Poisson distribution. Other than that, the steps are mostly the same as with the linear models you have seen before. To understand the estimate of the slope in a Poisson GLM we need to transform it back, e.g. from log (which is the natural logarithm in R) using the exponential function. This introduces a nonlinear behavior, a curve, and the slope now becomes the rate of the change of the tangent line to the curve. Question: A slope smaller than 1, would that result in increasing function (curve) or a decreasing function? What is overdispersion? 12.4 Binomial GLM We use the a binomial GLM if our response data consist of two level factor. It follow the same steps as the binomial GLM, however the intercepts and slopes are interpreted differently. In this first screencast we discuss the motivation behind using the binomial GLM. Questions: What is the main motivation to use binomial GLM? 12.4.1 Analysing a binomial GLM In analyzing GLM data we use the same steps as before. However, it become difficult to perform good model diagnostics, either due to the small number of observation because the number of success is reduced to one number or as the two-value nature of the response variable, as will be explained below. Question: If you prepare and experiment for the analysis with a binomial experiment, in what range of the numerical response variable should you have enough data? 12.4.2 Interpretation of the estimates The interpretation of slopes and intercepts are different as compared to the Poisson GLM. Now the response is interpreted as the probability of success and intercepts and slopes are related to odd ratios or risks what is the log odds-ratio? 12.5 Exercises: Choose the Probability Distribution A set of exercises can be downloaded here and its required data sets here. (If you can’t knit, click here for a PDF version of the exercises.) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
